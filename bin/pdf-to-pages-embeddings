#!/usr/bin/env ruby
# frozen_string_literal: true

require 'blingfire'
require 'csv'
require 'openai'
require 'pdf-reader'

BOOK = './static/the-real-frank-zappa-book.pdf'

TOKENIZER = BlingFire::Model.new

DOC_EMBEDDINGS_MODEL = 'text-search-curie-doc-001'

OPEN_AI_CLIENT = OpenAI::Client.new(access_token: ENV['OPEN_AI_ACCESS_TOKEN'])

def extract_pages(page, page_number)
    page_text = page.text
    if page_text.length == 0
        puts "empty page: #{page_number}"
        return {}
    end

    content = page_text.split().join(" ")
    {
        title: "Page #{page_number}",
        content: content,
        # 4 is a magic number.
        # source: https://github.com/slavingia/askmybook/blob/b0c0f6a/scripts/pdf_to_pages_embeddings.py#L50
        tokens: TOKENIZER.text_to_words(content).length + 4, 
    }
end

def exponential_backoff(&blk)
    retries = 0

    begin
        blk.call
    rescue => e
        sleep (2 ** retries)
        STDERR.print "."
        retries += 1
        retry
    end
end

def get_doc_embedding(text)
    # I'm getting rate limited. I can ask for a higher limit, I
    # guess. or I can try to rejigger the code so that it tries to
    # respect the 60 rpm rate limit...
    #
    # I can retry with a backoff.
    # https://help.openai.com/en/articles/7416438-rate-limits
    #
    # https://platform.openai.com/account/rate-limits
    result = OPEN_AI_CLIENT.embeddings(
        parameters: {
            model: DOC_EMBEDDINGS_MODEL,
            input: text
        }
    )

    return result["data"][0]["embedding"]
end

# Create an embedding for each row in the dataframe using the OpenAI Embeddings API.
# Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.
def compute_doc_embeddings(pages)
    pages.map do |page|
        exponential_backoff {
            STDERR.puts "getting embedding for page #{page[:title]}"
            get_doc_embedding(page[:content])
        }
    end
end

def main()
    reader = PDF::Reader.new(BOOK)
    pages =
        reader.
            pages.
            map.with_index(&method(:extract_pages)).
            filter do |page|
                # 2046 is a magic number.
                # source: https://github.com/slavingia/askmybook/blob/b0c0f6a/scripts/pdf_to_pages_embeddings.py#L70 
                page[:tokens] && page[:tokens] < 2046
            end

    pages_csv = "#{BOOK}.pages.csv"
    File.open(pages_csv, 'w') do |file|
        csv = CSV.new(file, headers: ["title", "content", "tokens"], write_headers: true)
        pages.each do |page|
            row = [ page[:title], page[:content], page[:tokens] ]
            csv << row
        end
    end

    doc_embeddings = compute_doc_embeddings(pages)

    embeddings_csv = "#{BOOK}.embeddings.csv"
    File.open(embeddings_csv, 'w') do |file|
        csv = CSV.new(file, headers: ["title", *(0..4095).to_a], write_headers: true)
        doc_embeddings.each_with_index do |embedding, index|
            # while
            # https://github.com/slavingia/askmybook/blob/b0c0f6a/scripts/pdf_to_pages_embeddings.py#L90
            # refers to this index as the "the row that it corresponds
            # to", further below, at
            # https://github.com/slavingia/askmybook/blob/b0c0f6a/scripts/pdf_to_pages_embeddings.py#L105
            # it's treated as a page number. I think that this is not
            # accurate since we've apparently already excluded pages
            # with more than 2046 tokens. I'm not sure what the intent
            # is, so I'm just gonna monkey-see-monkey-do for now.

            # this should honestly just correspond exactly to whatever
            # value in pages_csv.
            row = [ "Page #{(index)}", *embedding ]
            csv << row
        end
    end
end

main()
